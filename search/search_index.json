{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Next ALarM Sytem NALMS is an alarm system application designed for availability, integrability, and extensibility. The NALMS development was driven by SLAC's efforts to replace the Alarm Handler, due for deprecation as a Motif-based application, and to introduce process improvements addressing hierarchy implementation overhead, limited operator engagement, and operator display integration. Docker This repository is packaged with tools for Docker based deployment. There are several reasons containerization is an advantageous: The Kafka brokers may be straighforwardly deployed and the cluster scaled. Configurations are therefore transferable and port exposures may be configured directly on the Docker deployment. Contained applications may run in parallel, facilitating blue/green deployment workflows. This docker application consists of the following containers: Zookeeper Kafka Phoebus Alarm Server Phoebus Alarm Logger Elasticsearch Grafana An Example IOC Cruise Control Docker-compose may be used to run a packaged example with all components. $ docker-compose up Once running, the cruise control dashboard is available at http://localhost:9090, and the grafana alarm log dashboard is available at http://localhost:3000. The alarm logger may need to be restarted if using docker compose, due to a slight delay in the alarm server startup. Operations on the IOC for this demo can be performed by running caputs/cagets after attaching to the running container.","title":"Next ALarM Sytem"},{"location":"#next-alarm-sytem","text":"NALMS is an alarm system application designed for availability, integrability, and extensibility. The NALMS development was driven by SLAC's efforts to replace the Alarm Handler, due for deprecation as a Motif-based application, and to introduce process improvements addressing hierarchy implementation overhead, limited operator engagement, and operator display integration.","title":"Next ALarM Sytem"},{"location":"#docker","text":"This repository is packaged with tools for Docker based deployment. There are several reasons containerization is an advantageous: The Kafka brokers may be straighforwardly deployed and the cluster scaled. Configurations are therefore transferable and port exposures may be configured directly on the Docker deployment. Contained applications may run in parallel, facilitating blue/green deployment workflows. This docker application consists of the following containers: Zookeeper Kafka Phoebus Alarm Server Phoebus Alarm Logger Elasticsearch Grafana An Example IOC Cruise Control Docker-compose may be used to run a packaged example with all components. $ docker-compose up Once running, the cruise control dashboard is available at http://localhost:9090, and the grafana alarm log dashboard is available at http://localhost:3000. The alarm logger may need to be restarted if using docker compose, due to a slight delay in the alarm server startup. Operations on the IOC for this demo can be performed by running caputs/cagets after attaching to the running container.","title":"Docker"},{"location":"SLAC/","text":"SLAC Development On aird-b50-srv01, source the development environment: $ source ${ PACKAGE_TOP } /nalms/setup/aird-b50-srv01/dev.env","title":"SLAC"},{"location":"SLAC/#slac","text":"","title":"SLAC"},{"location":"SLAC/#development","text":"On aird-b50-srv01, source the development environment: $ source ${ PACKAGE_TOP } /nalms/setup/aird-b50-srv01/dev.env","title":"Development"},{"location":"cli/","text":"Command Line Tools In order to abstract the deployment process and interact with the Dockerized applications, a CLI has been packaged with the NALMS repository. A properly configured environment will define the following NALMS specific variables. EPICS variables must also be defined for appropriate networking. The nalms-tools package must be installed into the active python environment for Variable Description NALMS_KAFKA_PROPERTIES Path to Kafka properties file NALMS_ZOOKEEPER_PORT Zookeeper port NALMS_ES_PORT Elasticsearch port NALMS_CRUISE_CONTROL_PORT Cruise control port NALMS_KAFKA_PORT Exposed broker port NALMS_GRAFANA_PORT Grafana port NALMS_ES_HOST Elasticsearch network host address NALMS_KAFKA_BOOTSTRAP Bootstrap node for Kafka connections NALMS_ZOOKEEPER_HOST Zookeeper network host address NALMS_KAFKA_HOST Kafka broker network host address NALMS_CONFIGURATIONS Comma separated list of configurations for launching Grafana NALMS_HOME Path to NALMS repository NALMS_CLIENT_JAR Path to NALMS client jar file NALMS_ALARM_SERVER_PROPERTIES Path to alarm server properties file NALMS_ALARM_LOGGER_PROPERTIES Path to alarm logger properties file NALMS_CRUISE_CONTROL_PROPERTIES Path to cruise control properties file NALMS_GRAFANA_DASHBOARD_DIR Path to Grafana dashboard directory NALMS_GRAFANA_CONFIG Path to Grafana configuration file NALMS_GRAFANA_DATASOURCE_FILE Path to Grafana datasource file NALMS_ZOOKEEPER_CONFIG Path to Zookeeper configuration file NALMS_ES_CONFIG Path to elasticsearch configuration file For a multi-broker deployment, ports should be set on a per broker basis, divying the configuration script into separate scripts for each broker. convert-alh Command converts ALH configuration to Phoebus XML representation. $ bash cli/nalms convert-alh alh_file output_filename config_name delete-configuration Delete Kafka topics associated with a configuration. $ bash cli/nalms delete-configuration configuration_name generate-kafka-certs Generate certificates for Kafka trust store for configuration with SSL. $ bash cli/nalms generate-kafka-certs domain password launch-editor Launch the configuration editor. $ bash cli/nalms launch-editor list-configurations List active Kafka configurations. $ bash cli/nalms list-configurations start-alarm-logger Start the alarm logger for a given configuration name and configuration file. Configuration name must match that defined in the configuration file. This will create an image named nalms-logger-${CONFIG_NAME} . $ bash cli/nalms start-alarm-logger config_name config_file start-alarm-server Start the alarm server for a given configuration name and configuration file. Configuration name must match that defined in the configuration file. This will create an image named nalms-server-${CONFIG_NAME} . $ bash cli/nalms start-alarm-server config_name config_file start-cruise-control Start Cruise Control. This will create an image named nalms_cruise_control . $ bash cli/nalms start-cruise-control start-grafana Start the Grafana server, creating an image named nalms_grafana . $ bash cli/nalms start-grafana start-kafka-broker Start a Kafak broker. Must indicate a broker number for broker identification. This will create an image named nalms_kafka_$BROKER_NUMBER . $ bash cli/nalms start-kafka-broker --broker_number broker_number start-phoebus-client Launch the Phoebus client. $ bash cli/nalms start-phoebus-client start-zookeeper Start Zookeeper, creating image named nalms_zookeeper . $ bash cli/nalms start-zookeeper add-grafana-datasource Add a Grafana datasource for a configuration(s). $ bash cli/nalms add-grafana-datasource config_names build-grafana-dashboard Build a Grafana dashboard for a configuration $ bash cli/nalms build-grafana-dashboard config_name build-alarm-ioc Build alarm ioc files $ bash cli/nalms build-alarm-ioc app_name ioc_name config_name config_file target_architecture","title":"CLI"},{"location":"cli/#command-line-tools","text":"In order to abstract the deployment process and interact with the Dockerized applications, a CLI has been packaged with the NALMS repository. A properly configured environment will define the following NALMS specific variables. EPICS variables must also be defined for appropriate networking. The nalms-tools package must be installed into the active python environment for Variable Description NALMS_KAFKA_PROPERTIES Path to Kafka properties file NALMS_ZOOKEEPER_PORT Zookeeper port NALMS_ES_PORT Elasticsearch port NALMS_CRUISE_CONTROL_PORT Cruise control port NALMS_KAFKA_PORT Exposed broker port NALMS_GRAFANA_PORT Grafana port NALMS_ES_HOST Elasticsearch network host address NALMS_KAFKA_BOOTSTRAP Bootstrap node for Kafka connections NALMS_ZOOKEEPER_HOST Zookeeper network host address NALMS_KAFKA_HOST Kafka broker network host address NALMS_CONFIGURATIONS Comma separated list of configurations for launching Grafana NALMS_HOME Path to NALMS repository NALMS_CLIENT_JAR Path to NALMS client jar file NALMS_ALARM_SERVER_PROPERTIES Path to alarm server properties file NALMS_ALARM_LOGGER_PROPERTIES Path to alarm logger properties file NALMS_CRUISE_CONTROL_PROPERTIES Path to cruise control properties file NALMS_GRAFANA_DASHBOARD_DIR Path to Grafana dashboard directory NALMS_GRAFANA_CONFIG Path to Grafana configuration file NALMS_GRAFANA_DATASOURCE_FILE Path to Grafana datasource file NALMS_ZOOKEEPER_CONFIG Path to Zookeeper configuration file NALMS_ES_CONFIG Path to elasticsearch configuration file For a multi-broker deployment, ports should be set on a per broker basis, divying the configuration script into separate scripts for each broker.","title":"Command Line Tools"},{"location":"cli/#convert-alh","text":"Command converts ALH configuration to Phoebus XML representation. $ bash cli/nalms convert-alh alh_file output_filename config_name","title":"convert-alh"},{"location":"cli/#delete-configuration","text":"Delete Kafka topics associated with a configuration. $ bash cli/nalms delete-configuration configuration_name","title":"delete-configuration"},{"location":"cli/#generate-kafka-certs","text":"Generate certificates for Kafka trust store for configuration with SSL. $ bash cli/nalms generate-kafka-certs domain password","title":"generate-kafka-certs"},{"location":"cli/#launch-editor","text":"Launch the configuration editor. $ bash cli/nalms launch-editor","title":"launch-editor"},{"location":"cli/#list-configurations","text":"List active Kafka configurations. $ bash cli/nalms list-configurations","title":"list-configurations"},{"location":"cli/#start-alarm-logger","text":"Start the alarm logger for a given configuration name and configuration file. Configuration name must match that defined in the configuration file. This will create an image named nalms-logger-${CONFIG_NAME} . $ bash cli/nalms start-alarm-logger config_name config_file","title":"start-alarm-logger"},{"location":"cli/#start-alarm-server","text":"Start the alarm server for a given configuration name and configuration file. Configuration name must match that defined in the configuration file. This will create an image named nalms-server-${CONFIG_NAME} . $ bash cli/nalms start-alarm-server config_name config_file","title":"start-alarm-server"},{"location":"cli/#start-cruise-control","text":"Start Cruise Control. This will create an image named nalms_cruise_control . $ bash cli/nalms start-cruise-control","title":"start-cruise-control"},{"location":"cli/#start-grafana","text":"Start the Grafana server, creating an image named nalms_grafana . $ bash cli/nalms start-grafana","title":"start-grafana"},{"location":"cli/#start-kafka-broker","text":"Start a Kafak broker. Must indicate a broker number for broker identification. This will create an image named nalms_kafka_$BROKER_NUMBER . $ bash cli/nalms start-kafka-broker --broker_number broker_number","title":"start-kafka-broker"},{"location":"cli/#start-phoebus-client","text":"Launch the Phoebus client. $ bash cli/nalms start-phoebus-client","title":"start-phoebus-client"},{"location":"cli/#start-zookeeper","text":"Start Zookeeper, creating image named nalms_zookeeper . $ bash cli/nalms start-zookeeper","title":"start-zookeeper"},{"location":"cli/#add-grafana-datasource","text":"Add a Grafana datasource for a configuration(s). $ bash cli/nalms add-grafana-datasource config_names","title":"add-grafana-datasource"},{"location":"cli/#build-grafana-dashboard","text":"Build a Grafana dashboard for a configuration $ bash cli/nalms build-grafana-dashboard config_name","title":"build-grafana-dashboard"},{"location":"cli/#build-alarm-ioc","text":"Build alarm ioc files $ bash cli/nalms build-alarm-ioc app_name ioc_name config_name config_file target_architecture","title":"build-alarm-ioc"},{"location":"components/","text":"Components The NALMS alarm system is heavily modularized and consists of: Zookeeper for Kafka orchestration Kafka brokers for robust state management Cruise control for Kafka cluster administration Phoebus alarm server for the translation of EPICS alarm events into Kafka messages Phoebus alarm logger for the translation of Kafka messages into Elasticsearch documents A Grafana log dashboard for Elasticsearch integration PyDM tools for GUI integration AlarmIOC for exposure of alarm state to control system Due to this modularization, the system is easily extensible and all alarm state, configuration actions, and alarm actions are exposed over the Elasticsearch server API or via a Kafka consumer, easily integrable with applications beyond the scope of this project.","title":"Components"},{"location":"components/#components","text":"The NALMS alarm system is heavily modularized and consists of: Zookeeper for Kafka orchestration Kafka brokers for robust state management Cruise control for Kafka cluster administration Phoebus alarm server for the translation of EPICS alarm events into Kafka messages Phoebus alarm logger for the translation of Kafka messages into Elasticsearch documents A Grafana log dashboard for Elasticsearch integration PyDM tools for GUI integration AlarmIOC for exposure of alarm state to control system Due to this modularization, the system is easily extensible and all alarm state, configuration actions, and alarm actions are exposed over the Elasticsearch server API or via a Kafka consumer, easily integrable with applications beyond the scope of this project.","title":"Components"},{"location":"configuration/","text":"Configuration Alarm configurations are XML files organized with component (group), and PV tags. Component tags accept specifications for guidance, display, commands, and automated actions. Configuration options for groups are defined below: Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. PV tags accept specifications for enabling, latching, annunciating, description, delay, commands, associated displays, guidance, alarm count, filter, and automated actions. A configuration schema is provided here . Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. description Text displayed in the alarm table when the alarm is triggered delay Alarm will be triggered if the PV remains in alarm for at least this time enabled If false, ignore the value of this PV latching Alarms will latch to the highest severity until the alarm is acknowledged and cleared. If false, alarm may recover without requiring acknowledgment count If the trigger PV exhibits a not-OK alarm severity for more than \u2018count\u2019 times within the alarm delay, recognize the alarm filter An optional expression that can enable the alarm based on other PVs. Alarm Configuration Editor Tool The alarm configuration editor is be a PyQt tool for designing the alarm configuration XML files for use with the Phoebus alarm server as outlined in Section 3.2. Alternatively, any XML editor may be used to build the document directly. The editor has the following features:\u202f * Ability to edit alarm hierarchy, create new groups and new PVs * Ability to define all configuration items * Optional conversion and import of legacy ALH files Requirements for running the editor are given in the environment.yml file bundled with the NALMS package. This environment can be created with conda using: $ conda env create -f environment.yml And subsequently activated: $ conda activate nalms If choosing to build your own environment without conda, the requirements follow: - python =3.8 - treelib - lxml - pyqt5 - kafka-python - pydm PyDM dependence will eventually be dropped. To launch the editor run: $ bash cli/nalms launch-editor","title":"Configuration"},{"location":"configuration/#configuration","text":"Alarm configurations are XML files organized with component (group), and PV tags. Component tags accept specifications for guidance, display, commands, and automated actions. Configuration options for groups are defined below: Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. PV tags accept specifications for enabling, latching, annunciating, description, delay, commands, associated displays, guidance, alarm count, filter, and automated actions. A configuration schema is provided here . Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. description Text displayed in the alarm table when the alarm is triggered delay Alarm will be triggered if the PV remains in alarm for at least this time enabled If false, ignore the value of this PV latching Alarms will latch to the highest severity until the alarm is acknowledged and cleared. If false, alarm may recover without requiring acknowledgment count If the trigger PV exhibits a not-OK alarm severity for more than \u2018count\u2019 times within the alarm delay, recognize the alarm filter An optional expression that can enable the alarm based on other PVs.","title":"Configuration"},{"location":"configuration/#alarm-configuration-editor-tool","text":"The alarm configuration editor is be a PyQt tool for designing the alarm configuration XML files for use with the Phoebus alarm server as outlined in Section 3.2. Alternatively, any XML editor may be used to build the document directly. The editor has the following features:\u202f * Ability to edit alarm hierarchy, create new groups and new PVs * Ability to define all configuration items * Optional conversion and import of legacy ALH files Requirements for running the editor are given in the environment.yml file bundled with the NALMS package. This environment can be created with conda using: $ conda env create -f environment.yml And subsequently activated: $ conda activate nalms If choosing to build your own environment without conda, the requirements follow: - python =3.8 - treelib - lxml - pyqt5 - kafka-python - pydm PyDM dependence will eventually be dropped. To launch the editor run: $ bash cli/nalms launch-editor","title":"Alarm Configuration Editor Tool"},{"location":"demo/","text":"Demo The following PV tree will be used for the demo: We represent this tree with the configuration file in examples/demo/demo.xml . <?xml version='1.0' encoding='utf8'?> <config name=\"Demo\"> <component name=\"GROUP1\"> <pv name=\"DEMO:PV1\"> <enabled>true</enabled> <filter>DEMO:PV2 > 10</filter> </pv> <pv name=\"DEMO:PV2\"> <enabled>true</enabled> </pv> <pv name=\"DEMO:PV3\"> <enabled>true</enabled> </pv> </component> <component name=\"GROUP2\"> <component name=\"GROUP3\"> <pv name=\"DEMO:PV4\"> <enabled>true</enabled> </pv> <pv name=\"DEMO:PV5\"> <enabled>true</enabled> </pv> </component> <pv name=\"DEMO:PV6\"> <enabled>true</enabled> </pv> <pv name=\"DEMO:PV7\"> <enabled>true</enabled> </pv> </component> </config> Notice that DEMO:PV1 has an enabling filter based on the value of DEMO:PV2 . This results in a disabled DEMO:PV1 for values of DEMO:PV2 less than or equal to ten. This demo is intended for running on SLAC's RHEL7 dev server; however, this same demo can be executed on machines provided that an apropriate environment is sourced. NALMS uses named Docker containers and so this demo cannot be run if the existing demo containers are running. The commands below are run using the existing RHEL7 docker installation. Users must be added to the docker group in order to interact with the containers. For client use: $NALMS_CLIENT_JAR must be defined as well as $NALMS_HOME. The client launch script creates a templated configuration file for the client from a template provided in $NALMS_HOME. If running... If the service containers have already been deployed, you can access cruise control at: http://localhost:9090, the Grafana instance at: http://localhost:3000, and launch the client using: $ source /afs/slac/g/lcls/tools/script/ENVS64.bash $ source ${ PACKAGE_TOP } /nalms/setup/aird-b50-srv01/demo.env $ nalms start-phoebus-client Demo From scratch During this demo, we set up all services using the package CLI and the Docker images. On aird-b50-srv01, this can be sourced using: $ source /afs/slac/g/lcls/tools/script/ENVS64.bash $ source ${ PACKAGE_TOP } /nalms/setup/aird-b50-srv01/demo.env Demo IOC Start the demo ioc: $ tmux new -s demo-ioc $ softIoc -d ${ NALMS_HOME } /examples/demo/demo.db Exit the tmux window using: Ctr + b + d Alarm IOC For integration with edm/pydm displays etc., we can use the nalms cli to generate an alarm ioc for a given configuration. In this demo, we will use application name nalmsDemo , IOC name demo . $ source $EPICS_SETUP /epicsenv-7.0.3.1-1.0.bash $ nalms build-alarm-ioc nalmsDemo demo Demo ${ NALMS_HOME } /examples/demo/demo.xml rhel7-x86_64 $ cd nalmsDemo $ make $ cd iocBoot/iocdemo $ tmux new -s demoAlarmIOC # run in a new window $ ../../bin/rhel7-x86_64/nalmsDemo st.cmd $ dbl Exit the tmux window using: Ctr + b + d Kafka Cluster Set up Kafka cluster (from repo root): $ nalms start-zookeeper $ nalms start-kafka-broker --broker 0 Cruise Control Start cruise-control: $ nalms start-cruise-control Navigate to http://localhost:9090 to view the Cruise Control interface and monitors of the Kafka cluster. Phoebus Alarm Server Start the Phoebus alarm server: (Note: launch requires the absolute path of the configuration file for docker volume mount) $ nalms start-alarm-server Demo ${ NALMS_HOME } /examples/demo/demo.xml --alarmioc true Elasticsearch Next, start the Elasticsearch service: $ nalms start-elasticsearch Phoebus Alarm Logger Wait at least a minute before starting the Phoebus alarm logger. The templates for the indices must be created before starting. Start the Phoebus alarm logger: $ nalms start-alarm-logger Demo ${ NALMS_HOME } /examples/demo/demo.xml Grafana Now we'll add the configuration to be handled with Grafana. Add the Grafana datasource to the file: $ nalms add-grafana-datasource Demo This appended the datasource to the file datasource.yml . Now, create the Grafana dashboard: $ nalms build-grafana-dashboard Demo This created a dashboard for the Demo configuration in $NALMS_GRAFANA_DASHBOARD_DIR. Launch the Grafana instance: $ nalms start-grafana Launch firefox and navigate to http://localhost:3000 . Select AlarmLogs from the available dashboards. Phoebus Client Launch the Phoebus client $ nalms start-phoebus-client Demo Navigate to Applications > Alarm > Alarm Tree to view the process variable values. Navigate to Applications > Alarm > Alarm Log Inspect To inspect the Docker containers run: $ docker ps # to list container ids $ docker stats { CONTAINER_ID } Cleanup All containers may be stopped using the ids listed with: $ docker ps $ docker stop { containter_id } Remove lingering containers... $ docker container rm {container_id} You can access and exit the demo ioc by attaching to the tmux session: $ tmux attach -t demo-ioc Exiting: $ > exit $ exit And the alarm ioc: $ tmux attach -t demoAlarmIoc Exiting: $ > exit $ exit","title":"Demo"},{"location":"demo/#demo","text":"The following PV tree will be used for the demo: We represent this tree with the configuration file in examples/demo/demo.xml . <?xml version='1.0' encoding='utf8'?> <config name=\"Demo\"> <component name=\"GROUP1\"> <pv name=\"DEMO:PV1\"> <enabled>true</enabled> <filter>DEMO:PV2 > 10</filter> </pv> <pv name=\"DEMO:PV2\"> <enabled>true</enabled> </pv> <pv name=\"DEMO:PV3\"> <enabled>true</enabled> </pv> </component> <component name=\"GROUP2\"> <component name=\"GROUP3\"> <pv name=\"DEMO:PV4\"> <enabled>true</enabled> </pv> <pv name=\"DEMO:PV5\"> <enabled>true</enabled> </pv> </component> <pv name=\"DEMO:PV6\"> <enabled>true</enabled> </pv> <pv name=\"DEMO:PV7\"> <enabled>true</enabled> </pv> </component> </config> Notice that DEMO:PV1 has an enabling filter based on the value of DEMO:PV2 . This results in a disabled DEMO:PV1 for values of DEMO:PV2 less than or equal to ten. This demo is intended for running on SLAC's RHEL7 dev server; however, this same demo can be executed on machines provided that an apropriate environment is sourced. NALMS uses named Docker containers and so this demo cannot be run if the existing demo containers are running. The commands below are run using the existing RHEL7 docker installation. Users must be added to the docker group in order to interact with the containers. For client use: $NALMS_CLIENT_JAR must be defined as well as $NALMS_HOME. The client launch script creates a templated configuration file for the client from a template provided in $NALMS_HOME.","title":"Demo"},{"location":"demo/#if-running","text":"If the service containers have already been deployed, you can access cruise control at: http://localhost:9090, the Grafana instance at: http://localhost:3000, and launch the client using: $ source /afs/slac/g/lcls/tools/script/ENVS64.bash $ source ${ PACKAGE_TOP } /nalms/setup/aird-b50-srv01/demo.env $ nalms start-phoebus-client Demo","title":"If running..."},{"location":"demo/#from-scratch","text":"During this demo, we set up all services using the package CLI and the Docker images. On aird-b50-srv01, this can be sourced using: $ source /afs/slac/g/lcls/tools/script/ENVS64.bash $ source ${ PACKAGE_TOP } /nalms/setup/aird-b50-srv01/demo.env","title":"From scratch"},{"location":"demo/#demo-ioc","text":"Start the demo ioc: $ tmux new -s demo-ioc $ softIoc -d ${ NALMS_HOME } /examples/demo/demo.db Exit the tmux window using: Ctr + b + d","title":"Demo IOC"},{"location":"demo/#alarm-ioc","text":"For integration with edm/pydm displays etc., we can use the nalms cli to generate an alarm ioc for a given configuration. In this demo, we will use application name nalmsDemo , IOC name demo . $ source $EPICS_SETUP /epicsenv-7.0.3.1-1.0.bash $ nalms build-alarm-ioc nalmsDemo demo Demo ${ NALMS_HOME } /examples/demo/demo.xml rhel7-x86_64 $ cd nalmsDemo $ make $ cd iocBoot/iocdemo $ tmux new -s demoAlarmIOC # run in a new window $ ../../bin/rhel7-x86_64/nalmsDemo st.cmd $ dbl Exit the tmux window using: Ctr + b + d","title":"Alarm IOC"},{"location":"demo/#kafka-cluster","text":"Set up Kafka cluster (from repo root): $ nalms start-zookeeper $ nalms start-kafka-broker --broker 0","title":"Kafka Cluster"},{"location":"demo/#cruise-control","text":"Start cruise-control: $ nalms start-cruise-control Navigate to http://localhost:9090 to view the Cruise Control interface and monitors of the Kafka cluster.","title":"Cruise Control"},{"location":"demo/#phoebus-alarm-server","text":"Start the Phoebus alarm server: (Note: launch requires the absolute path of the configuration file for docker volume mount) $ nalms start-alarm-server Demo ${ NALMS_HOME } /examples/demo/demo.xml --alarmioc true","title":"Phoebus Alarm Server"},{"location":"demo/#elasticsearch","text":"Next, start the Elasticsearch service: $ nalms start-elasticsearch","title":"Elasticsearch"},{"location":"demo/#phoebus-alarm-logger","text":"Wait at least a minute before starting the Phoebus alarm logger. The templates for the indices must be created before starting. Start the Phoebus alarm logger: $ nalms start-alarm-logger Demo ${ NALMS_HOME } /examples/demo/demo.xml","title":"Phoebus Alarm Logger"},{"location":"demo/#grafana","text":"Now we'll add the configuration to be handled with Grafana. Add the Grafana datasource to the file: $ nalms add-grafana-datasource Demo This appended the datasource to the file datasource.yml . Now, create the Grafana dashboard: $ nalms build-grafana-dashboard Demo This created a dashboard for the Demo configuration in $NALMS_GRAFANA_DASHBOARD_DIR. Launch the Grafana instance: $ nalms start-grafana Launch firefox and navigate to http://localhost:3000 . Select AlarmLogs from the available dashboards.","title":"Grafana"},{"location":"demo/#phoebus-client","text":"Launch the Phoebus client $ nalms start-phoebus-client Demo Navigate to Applications > Alarm > Alarm Tree to view the process variable values. Navigate to Applications > Alarm > Alarm Log","title":"Phoebus Client"},{"location":"demo/#inspect","text":"To inspect the Docker containers run: $ docker ps # to list container ids $ docker stats { CONTAINER_ID }","title":"Inspect"},{"location":"demo/#cleanup","text":"All containers may be stopped using the ids listed with: $ docker ps $ docker stop { containter_id } Remove lingering containers... $ docker container rm {container_id} You can access and exit the demo ioc by attaching to the tmux session: $ tmux attach -t demo-ioc Exiting: $ > exit $ exit And the alarm ioc: $ tmux attach -t demoAlarmIoc Exiting: $ > exit $ exit","title":"Cleanup"},{"location":"development/","text":"Docs This project uses mkdocs for generating documentation. This can be installed with Python. Once mkdocs and mkdocs-material have been installed, the documentation may be served locally using the command: $ mkdocs serve A GitHub action workflow has been configured such that the docs are automatically created on merge to the slaclab/nalms main branch. Docker images Significant simplifications might be made to these docker images (moving to more modern OS etc.); however, I've tried to replicate the RHEL 7 design requirement as closely as possible to demonstrate the installation outlined in the design document. Newer versioned releases should be indicated to the nalms package by updates to version environment variables: # versions export NALMS_DOCKER_ES_VERSION = v1 . 0 export NALMS_DOCKER_GRAFANA_VERSION = v1 . 0 export NALMS_DOCKER_ALARM_SERVER_VERSION = v1 . 0 export NALMS_DOCKER_ALARM_LOGGER_VERSION = v1 . 0 export NALMS_DOCKER_ZOOKEEPER_VERSION = v1 . 0 export NALMS_DOCKER_CRUISE_CONTROL_VERSION = v1 . 0 export NALMS_DOCKER_KAFKA_VERSION = v1 . 0 Useful commands: To see all running and stopped containers $ sudo docker container ls -a The current installation of Docker on rhel... requires sudo for use to get the id of a running container: $ sudo docker container ls To get memory, CPU use: sudo docker container stats ${ CONTAINER_ID } Container information, including networking info, is available using: $ sudo docker container inspect ${ CONTAINER_ID } To attach to a running container: $ sudo docker container exec -it fdc6ce9ce655 /bin/bash DockerHub deployment All Dockerhub images are hosted on the TID Advanced Controls Systems account (tidacs). A Github action has been defined for the automatic build of images on tagged releases to the main slaclab/master branch. All images should be eventually removed from this repository and moved into their own for convenient versioning. Ongoing projects An attempt has been made to document development needs using Github projects here . Helpful debugging Phoebus Alarm Server In the event of problematic IOC connectivity, it may be worthwhile to connect to the Phoebus Alarm Server docker container using: $ sudo docker container exec -it fdc6ce9ce655 /bin/bash and edit the logging options in $LOGGING_CONFIG_FILE. org.phoebus.applications.alarm.level = INFO com.cosylab.epics.caj.level = FINE # handles connectivity org.phoebus.framework.rdb.level = WARNING org.phoebus.pv.level = FINE org.apache.kafka.level = SEVERE Grafana template In order to use the Grafana dashboard with a scaling number of configurations, a json template is located in grafana/dashboards/alarm_logs_dashboard.json . For further development of the dashboard, the template must be changed using a local Grafana instance. Steps for updating are: * Copy json representation * Remove id from the json representation * Replace datasource name entries from json representation with environment var placeholder $DATASOURCE_NAME","title":"Development"},{"location":"development/#docs","text":"This project uses mkdocs for generating documentation. This can be installed with Python. Once mkdocs and mkdocs-material have been installed, the documentation may be served locally using the command: $ mkdocs serve A GitHub action workflow has been configured such that the docs are automatically created on merge to the slaclab/nalms main branch.","title":"Docs"},{"location":"development/#docker-images","text":"Significant simplifications might be made to these docker images (moving to more modern OS etc.); however, I've tried to replicate the RHEL 7 design requirement as closely as possible to demonstrate the installation outlined in the design document. Newer versioned releases should be indicated to the nalms package by updates to version environment variables: # versions export NALMS_DOCKER_ES_VERSION = v1 . 0 export NALMS_DOCKER_GRAFANA_VERSION = v1 . 0 export NALMS_DOCKER_ALARM_SERVER_VERSION = v1 . 0 export NALMS_DOCKER_ALARM_LOGGER_VERSION = v1 . 0 export NALMS_DOCKER_ZOOKEEPER_VERSION = v1 . 0 export NALMS_DOCKER_CRUISE_CONTROL_VERSION = v1 . 0 export NALMS_DOCKER_KAFKA_VERSION = v1 . 0","title":"Docker images"},{"location":"development/#useful-commands","text":"To see all running and stopped containers $ sudo docker container ls -a The current installation of Docker on rhel... requires sudo for use to get the id of a running container: $ sudo docker container ls To get memory, CPU use: sudo docker container stats ${ CONTAINER_ID } Container information, including networking info, is available using: $ sudo docker container inspect ${ CONTAINER_ID } To attach to a running container: $ sudo docker container exec -it fdc6ce9ce655 /bin/bash","title":"Useful commands:"},{"location":"development/#dockerhub-deployment","text":"All Dockerhub images are hosted on the TID Advanced Controls Systems account (tidacs). A Github action has been defined for the automatic build of images on tagged releases to the main slaclab/master branch. All images should be eventually removed from this repository and moved into their own for convenient versioning.","title":"DockerHub deployment"},{"location":"development/#ongoing-projects","text":"An attempt has been made to document development needs using Github projects here .","title":"Ongoing projects"},{"location":"development/#helpful-debugging","text":"","title":"Helpful debugging"},{"location":"development/#phoebus-alarm-server","text":"In the event of problematic IOC connectivity, it may be worthwhile to connect to the Phoebus Alarm Server docker container using: $ sudo docker container exec -it fdc6ce9ce655 /bin/bash and edit the logging options in $LOGGING_CONFIG_FILE. org.phoebus.applications.alarm.level = INFO com.cosylab.epics.caj.level = FINE # handles connectivity org.phoebus.framework.rdb.level = WARNING org.phoebus.pv.level = FINE org.apache.kafka.level = SEVERE","title":"Phoebus Alarm Server"},{"location":"development/#grafana-template","text":"In order to use the Grafana dashboard with a scaling number of configurations, a json template is located in grafana/dashboards/alarm_logs_dashboard.json . For further development of the dashboard, the template must be changed using a local Grafana instance. Steps for updating are: * Copy json representation * Remove id from the json representation * Replace datasource name entries from json representation with environment var placeholder $DATASOURCE_NAME","title":"Grafana template"},{"location":"epics_integration/","text":"EPICS integration This repository is currently outfitted to integrate directly with SLAC's alarm IOC as used in conjunction with the legacy Alarm Handler. This is critical for a seamless transition between the ALH and NALMS system and will allow for the surfacing of EDM/PyDM alarming indicators (bypass markers) while using the new system. In addition to the bypass indicator, an acknowledgment indicator has also been added to the NALMS alarm IOC, allowing acknowledgment status to be indicated in displays just as bypasses. Postfixes are applied to the end of pv names to create new these new indicator pvs. The alarm ioc consists of: FP postifixed variables for indicating bypass, written to by system SV posftixed variables for scanning bypass indicator DP postfixed variables for propogating the alarm system's effective PV severity Group level STATSUMY postfixed variables for the propogation of the alarm system's effective group severity Group level STATSUMYFP postfixed variables for the propogation of the alarm system's effective bypass status Inside the nalms-phoebus-alarm-server Docker image, the script: phoebus-alarm-server/scripts/update-ioc.py continually runs and updates the alarm ioc with the pvs' bypass states. EPICS environment variables may be passed to the image to configure EPICS. Generation Constructed using a template found in nalms-tools/..","title":"EPICS Integration"},{"location":"epics_integration/#epics-integration","text":"This repository is currently outfitted to integrate directly with SLAC's alarm IOC as used in conjunction with the legacy Alarm Handler. This is critical for a seamless transition between the ALH and NALMS system and will allow for the surfacing of EDM/PyDM alarming indicators (bypass markers) while using the new system. In addition to the bypass indicator, an acknowledgment indicator has also been added to the NALMS alarm IOC, allowing acknowledgment status to be indicated in displays just as bypasses. Postfixes are applied to the end of pv names to create new these new indicator pvs. The alarm ioc consists of: FP postifixed variables for indicating bypass, written to by system SV posftixed variables for scanning bypass indicator DP postfixed variables for propogating the alarm system's effective PV severity Group level STATSUMY postfixed variables for the propogation of the alarm system's effective group severity Group level STATSUMYFP postfixed variables for the propogation of the alarm system's effective bypass status Inside the nalms-phoebus-alarm-server Docker image, the script: phoebus-alarm-server/scripts/update-ioc.py continually runs and updates the alarm ioc with the pvs' bypass states. EPICS environment variables may be passed to the image to configure EPICS.","title":"EPICS integration"},{"location":"epics_integration/#generation","text":"Constructed using a template found in nalms-tools/..","title":"Generation"},{"location":"install/","text":"Installation The NALMS system has been written for deployment as a set of Docker images, allowing for the distribution of services across network hosts configurable via environment variables and mounted volumes. In the case that containers aren't favorable, the Dockerfiles inside this repository should serve as a guide for these deployments and component source materials should be consulted for installation details. The current NALMS iteration consists of the following Dockerhub hosted containers: tidacs/nalms-zookeeper Zookeeper 3.5.9 tidacs/nalms-kafka Kafka 2.8.1 configured with cruise-control metrics reporter SSL configurable (see networking ) tidacs/nalms-phoebus-alarm-server Phoebus alarm server (built from HEAD of main branch) Python script for monitoring Kafka topics updating alarm IOC with bypasses and acknowledgments tidacs/nalms-phoebus-alarm-logger Phoebus alarm logger (built from HEAD of main branch) tidacs/nalms-elasticsearch Elasticsearch service (6.8.22) Script for templating of Alarm indices tidacs/nalms-grafana Grafana service (7.5.3) Template Grafana dashboard for any configuration Can be launched with multiple configurations as a comma separated list Automatic generation of elasticsearch datasources based on network configs and configuration names tidacs/nalms-cruise-control LinkendIn's Cruise Control monitor for Kafka clusters (built from HEAD of branch migrate_to_kafka_2_5, which is compatible with Kafka 2.7.0) Cruise Control web UI Download Images Images may be downloaded from Dockerhub on a machine with Docker installed using the command (nalms-kafka here for example): $ docker pull tidacs/nalms-kafka:latest Client installation At present, we will build the client from the HEAD of the main branch of the Phoebus client hosted on Github. Requirements: OpenJDK == 11.0.2 Maven == 3.6.0 Git >= 1.8 (likely compatable with lower, but only tested as low as 1.8.3) Installation: Set $JAVA_HOME , $MAVEN_HOME , and $NALMS_HOME . Then update the path: $ export PATH=$JAVA_HOME/bin:$PATH $ export PATH=$MAVEN_HOME/bin:$PATH Note: In afs, JAVA_HOME=${PACKAGE_TOP}/java/jdk-11.0.2, MAVEN_HOME=${PACKAGE_TOP}/maven/3.6.0, and NALMS_TOP=${PACKAGE_TOP}/nalms/current. cd into installation directory Get Phoebus repository $ git clone https://github.com/ControlSystemStudio/phoebus.git $ cd phoebus Now replace the existing product pom file with that packaged by NALMS: $ rm phoebus-product/pom.xml $ mv $NALMS_HOME/phoebus-client/pom.xml phoebus-product/pom.xml Install the Phoebus client: $ mvn install -pl phoebus-product -am Define $NALMS_CLIENT_JAR in appropriate environment file. Deploy After pulling the latest image, it is recommended to use the CLI for launching of each image as checks for necessary environment variables are built in to the interface. A full description of each image configuration is described here . Modify cli/nalms for execution: $ chmod +x cli/nalms Configuration This sections includes an attempt to address configuration items, giving some insight to the service configuration within the Dockerized components and their Docker arguments. An attempt has been made to sufficiently abstract scripts for deployments based on environment variables and a full descriptions of a complete environment for deployment is giving in the CLI documentation. Below, each component is outline with respect to Docker configuration variables and configuration file structure. For a full resource of available configurations, the source documentation will be linked. Elasticsearch Configuration The Elasticsearch configuration consists of three main files: elasticsearch.yml for configuring Elasticsearch jvm.options for configuring Elasticsearch JVM settings log4j2.properties for configuring Elasticsearch logging A reference for elasticsearch configuration files can be found here . In order for the Elasticsearch fields to be properly formatted, a template matching the topic scheme must be posted to the server. These may be versioned and are automatically applied to newly created indices. The initial script for templating NALMS topics is hosted in elasticsearch/scripts/create_alarm_template.sh . This template has been taken from the Phoebus source examples . Docker The elasticsearch node may be configured using an exposed port, node specific variables, and Kafka networking variables. Because this is a single node deployment, single-node deployment is used. Java options may be specifified using the ES_JAVA_OPTS variable. The Elasticsearch docker image also creates the appropriate elasticsearch template for the configuration messages. The configuration files must be mounted to /usr/share/elasticsearch/config . The following Docker run command will lauch an Elasticsearch node reachable on host machine port 9200. $ docker run \\ -e node.name = node01 \\ -e cluster.name = es-cluster-7 \\ -e discovery.type = single-node \\ -e ES_JAVA_OPTS = \"-Xms128m -Xmx128m\" \\ -e ES_HOST = localhost \\ -e ES_PORT = 9200 \\ -v \" ${ NALMS_ES_CONFIG } :/usr/share/elasticsearch/config\" \\ -p \" $NALMS_ES_PORT :9200\" \\ --name nalms_elasticsearch \\ -d tidacs/nalms-elasticsearch:latest Zookeeper Configuration At present, Zookeeper is launched using the default settings. For more sophisticated deployments, a configuration with mounted configuration files would be preferable. The configuration file is mounted to the Zookeeper container at runtime. A description of the zookeeper configuration may be found here . Docker The following command will run Zookeeper accessible on the host machine at port 2181: $ docker run -p \" ${ NALMS_ZOOKEEPER_PORT } :2181\" -e ZOOKEEPER_CONFIG = /tmp/zoo.cfg \\ -v \" ${ NALMS_ZOOKEEPER_CONFIG } :/tmp/zoo.cfg\" --name nalms_zookeeper \\ -d tidacs/nalms-zookeeper:latest Kafka Configuration This file is used to configure general properties of a Kafka broker including replication settings and communications protocols. Listeners are defined with respect to configured protocols and binding ports. Advertised listeners are configured with respect to configured protocol and exposed ports. The replication.factor must be appropriately modified based off of the number of nodes in the deployment. A single broker deployment would require replication.factor set to 1. A cluster deployment can accomodate a larger replication factor across the cluster and this file must be modified for the purpose. Networking configurations for SSL/TLS configuration settings are described here . Also defined in this file is the reference to the zookeeper docker image resource: zookeeper . connect = zookeeper : 2181 Certain configurations options may be defined on the topic level. In phoebus-alarm-server/cli/commands/create-kafka-indices , state topics are created with partitions and replications dependent on the cluster settings. After initial creation, the Talk and Command topics are modified to use the deletion cleanup policy with set retention time. At present, the Talk command unused. The create-kafka-indices command is automatically executed during alarm server docker image startup. There are many other settings pertaining to the optimization of the cluster and must be determined by traffic demands. A full catalog of available configurations may be found in the documentation, here . Docker The Kafka broker images require the definition of Kafka networking variables, KAFKA_ADVERTISED_LISTENERS , KAFKA_LISTENER_SECURITY_PROTOCOL_MAP , KAFKA_LISTENERS , ZOOKEEPER_CONNECT and must be provided a numeric broker ID. The image must be provisioned with an 8g memory allocation. Additional optimizations may be performed using the Docker image configurations . A configuration file must be mounted to /opt/kafka/server.properties for the image, with properly formatted networking and replication numbers. An example server configuration is given in examples/demo/config/server.properties . An example run command for the Kafka docker image is given below: $ docker run -m 8g \\ -e KAFKA_ADVERTISED_LISTENERS = PLAINTEXT:// ${ HOST_IP } :9092,CONNECTIONS_FROM_HOST:// ${ HOST_IP } :19092 \\ -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP = PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT \\ -e KAFKA_LISTENERS = PLAINTEXT:// ${ HOST_IP } :9092,CONNECTIONS_FROM_HOST://0.0.0.0:19092 \\ -e ZOOKEEPER_CONNECT = ${ HOST_IP } :2182 \\ -e BROKER_ID = 0 \\ -v \"/full/path/to/examples/demo/config/server.properties:/opt/kafka/server.properties\" \\ -p \"19092:19092\" \\ --name nalms_kafka_0 \\ -d tidacs/nalms-kafka:latest Instructions on configuring the Docker image with SSL are given in networking . Phoebus Alarm Server Configuration The Phoebus alarm server configuration properties file defines the EPICS configuration and Elasticsearch host configuration. A full preference list can be found in the CS-Studio documentation . Of particular importance are the EPICS, Kakfa, and Elasticsearch properties. The Alarm Server requires: # Channel access settings org.phoebus.pv.ca/addr_list = localhost:5064 org.phoebus.pv.ca/server_port = 5064 org.phoebus.pv.ca/repeater_port = 5065 org.phoebus.pv.ca/auto_addr_list = no # pvAccess settings # Kafka org.phoebus.applications.alarm/server = kafka:19092 Docker The Phoebus alarm server requires mounting of the configuration file with the Docker volume option and the definition of environment variables indicating Kafka networking address, whether the alarm IOC is to be used, and the EPICS configuration settings to access the alarm and variable iocs. The image supports the substitution of networking variables ($KAFKA_BOOTSTRAP and EPICS variables). Alternatively, these can be defined directly in the configuration file. The Docker run command for the packaged example is given below: $ docker run -v $CONFIG_FILE :/tmp/nalms/ $CONFIG_NAME .xml \\ --name nalms_server_ $CONFIG_NAME \\ -v \" ${ NALMS_ALARM_SERVER_PROPERTIES } :/opt/nalms/config/alarm_server.properties\" \\ -e ALARM_IOC = false \\ -e KAFKA_BOOTSTRAP = \" ${ NALMS_KAFKA_BOOTSTRAP } \" \\ -e EPICS_CA_ADDR_LIST = \" ${ EPICS_CA_ADDR_LIST } \" \\ -e EPICS_CA_SERVER_PORT = \" ${ EPICS_CA_SERVER_PORT } \" \\ -e EPICS_CA_REPEATER_PORT = \" ${ EPICS_CA_REPEATER_PORT } \" \\ -e EPICS_PVA_ADDR_LIST = \" ${ EPICS_PVA_ADDR_LIST } \" \\ -e EPICS_PVA_SERVER_PORT = \" ${ EPICS_PVA_SERVER_PORT } \" \\ -e EPICS_PVA_REPEATER_PORT = \" ${ EPICS_PVA_REPEATER_PORT } \" \\ -e ALARM_SERVER_PROPERTIES = \"/opt/nalms/config/alarm_server.properties\" \\ -d -t tidacs/nalms-phoebus-alarm-server:latest start-server $CONFIG_NAME /tmp/nalms/ $CONFIG_NAME .xml The configuration file must be mounted to `/tmp/nalms/${CONFIG_NAME}, for internal identification. Phoebus Alarm Logger Configuration The alarm logger properties file requires the definition of Elasticsearch and Kafka networking environment variables. The templated file used by the image is hosted at phoebus-alarm-logger/logger.properties . # location of elastic node/s es_host = localhost es_port = 9200 # Kafka server location bootstrap.servers = localhost:9092 Additionally, logging for the logger is configurable and defined in phoebus-alarm-server/logger.properties . Docker The Phoebus alarm logger requires the mounting of the configuration file with the Docker volume option. The image supports the interpolation of networking variables $NALMS_ES_HOST, $NALMS_ES_PORT, and $NALMS_KAFKA_BOOTSTRAP in this file. The Docker run command for the packaged example is given below: $ docker run -v $CONFIG_FILE :/tmp/nalms/ $CONFIG_NAME .xml \\ -e ES_HOST = \" ${ NALMS_ES_HOST } \" \\ -e ES_PORT = \" ${ NALMS_ES_PORT } \" \\ -e BOOTSTRAP_SERVERS = \" ${ NALMS_KAFKA_BOOTSTRAP } \" \\ -e ALARM_LOGGER_PROPERTIES = \"/opt/nalms/config/alarm_logger.properties\" \\ -v \" ${ ALARM_LOGGER_PROPERTIES } :/opt/nalms/config/alarm_logger.properties\" \\ --name nalms_logger_ $CONFIG_NAME \\ -d tidacs/nalms-phoebus-alarm-logger:latest start-logger $CONFIG_NAME /tmp/nalms/ $CONFIG_NAME .xml The configuration file must be mounted to `/tmp/nalms/${CONFIG_NAME}, for internal identification. Phoebus Client Configuration Like the alarm server and logger, the client also accepts a properties file that defines networking: here . Grafana Configuration Grafana datasources and dashboards may be programatically provisioned as outlined here . Elasticsearch datasources define an index and networking variables. General Grafana configuration is described here . The dashboard template is hosted at grafana/dashboards/alarm_logs_dashboard.json and a configuration dashboard can be created using the cli/nalms build-grafana-dashboard config-name command. The datasource may be added to an existing datasource file using the cli/nalms add-grafana-datasource config-name command or manually created. Docker The Grafana image requires mounting of the dashboards, datasource file, and configuration file. The Docker run command for the packaged example is given below: $ docker run \\ -p \" ${ NALMS_GRAFANA_PORT } :3000\" \\ -v \" ${ NALMS_GRAFANA_DASHBOARD_DIR } :/var/lib/grafana/dashboards\" \\ -v \" ${ NALMS_GRAFANA_DATASOURCE_FILE } :/etc/grafana/provisioning/datasources/all.yml\" \\ -v \" ${ NALMS_GRAFANA_CONFIG } :/etc/grafana/config.ini\" \\ -e ES_HOST = $NALMS_ES_HOST \\ -e ES_PORT = $NALMS_ES_PORT \\ --name nalms_grafana \\ -d tidacs/nalms-grafana:latest The datasource file must be mounted to /etc/grafana/provisioning/datasources/all.yml , the dashboard directory must be mounted to /var/lib/grafana/dashboards , and the configuration must be mounted to /etc/grafana/provisioning/datasources/all.yml . The Grafana dashboards are then reachable at localhost:${NALMS_GRAFANA_PORT} in browser. Cruise Control Configuration The cruise-control/cruisecontrol.properties file dictates the behavior of the cruise control server, allowing definition of relevant thresholds and networking nodes. The tidacs/nalms-cruise-control image performs interpolation on this file in order to pass the relevant environment variables. See wiki: https://github.com/linkedin/cruise-control/wiki https://github.com/linkedin/cruise-control-ui/wiki/Single-Kafka-Cluster Docker In order to run this image, you must mount a cruisecontrol.properties to a path specified with the $CRUISE_CONTROL_PROPERTIES env variable. The image will perform interpolation on properties files with $BOOTSTRAP_SERVERS or $ZOOKEEPER_CONNECT as placeholders and defined $BOOTSTRAP_SERVERS or $ZOOKEEPER_CONNECT environment variables. The Docker run command for the packaged example is given below: $ docker run \\ -e BOOTSTRAP_SERVERS = \" ${ NALMS_KAFKA_BOOTSTRAP } \" \\ -e ZOOKEEPER_CONNECT = \" ${ NALMS_ZOOKEEPER_HOST } : ${ NALMS_ZOOKEEPER_PORT } \" \\ -e CRUISE_CONTROL_PROPERTIES = \"/opt/cruise-control/config/cruisecontrol.properties\" \\ -v \" ${ NALMS_CRUISE_CONTROL_PROPERTIES } :/opt/cruise-control/config/cruisecontrol.properties\" \\ --name nalms_cruise_control \\ -p \" $NALMS_CRUISE_CONTROL_PORT :9090\" -d tidacs/nalms-cruise-control:latest The Cruise Control UI is then available in browser at localhost:9090.","title":"Installation"},{"location":"install/#installation","text":"The NALMS system has been written for deployment as a set of Docker images, allowing for the distribution of services across network hosts configurable via environment variables and mounted volumes. In the case that containers aren't favorable, the Dockerfiles inside this repository should serve as a guide for these deployments and component source materials should be consulted for installation details. The current NALMS iteration consists of the following Dockerhub hosted containers:","title":"Installation"},{"location":"install/#tidacsnalms-zookeeper","text":"Zookeeper 3.5.9","title":"tidacs/nalms-zookeeper"},{"location":"install/#tidacsnalms-kafka","text":"Kafka 2.8.1 configured with cruise-control metrics reporter SSL configurable (see networking )","title":"tidacs/nalms-kafka"},{"location":"install/#tidacsnalms-phoebus-alarm-server","text":"Phoebus alarm server (built from HEAD of main branch) Python script for monitoring Kafka topics updating alarm IOC with bypasses and acknowledgments","title":"tidacs/nalms-phoebus-alarm-server"},{"location":"install/#tidacsnalms-phoebus-alarm-logger","text":"Phoebus alarm logger (built from HEAD of main branch)","title":"tidacs/nalms-phoebus-alarm-logger"},{"location":"install/#tidacsnalms-elasticsearch","text":"Elasticsearch service (6.8.22) Script for templating of Alarm indices","title":"tidacs/nalms-elasticsearch"},{"location":"install/#tidacsnalms-grafana","text":"Grafana service (7.5.3) Template Grafana dashboard for any configuration Can be launched with multiple configurations as a comma separated list Automatic generation of elasticsearch datasources based on network configs and configuration names","title":"tidacs/nalms-grafana"},{"location":"install/#tidacsnalms-cruise-control","text":"LinkendIn's Cruise Control monitor for Kafka clusters (built from HEAD of branch migrate_to_kafka_2_5, which is compatible with Kafka 2.7.0) Cruise Control web UI","title":"tidacs/nalms-cruise-control"},{"location":"install/#download-images","text":"Images may be downloaded from Dockerhub on a machine with Docker installed using the command (nalms-kafka here for example): $ docker pull tidacs/nalms-kafka:latest","title":"Download Images"},{"location":"install/#client-installation","text":"At present, we will build the client from the HEAD of the main branch of the Phoebus client hosted on Github. Requirements: OpenJDK == 11.0.2 Maven == 3.6.0 Git >= 1.8 (likely compatable with lower, but only tested as low as 1.8.3)","title":"Client installation"},{"location":"install/#installation_1","text":"Set $JAVA_HOME , $MAVEN_HOME , and $NALMS_HOME . Then update the path: $ export PATH=$JAVA_HOME/bin:$PATH $ export PATH=$MAVEN_HOME/bin:$PATH Note: In afs, JAVA_HOME=${PACKAGE_TOP}/java/jdk-11.0.2, MAVEN_HOME=${PACKAGE_TOP}/maven/3.6.0, and NALMS_TOP=${PACKAGE_TOP}/nalms/current. cd into installation directory Get Phoebus repository $ git clone https://github.com/ControlSystemStudio/phoebus.git $ cd phoebus Now replace the existing product pom file with that packaged by NALMS: $ rm phoebus-product/pom.xml $ mv $NALMS_HOME/phoebus-client/pom.xml phoebus-product/pom.xml Install the Phoebus client: $ mvn install -pl phoebus-product -am Define $NALMS_CLIENT_JAR in appropriate environment file.","title":"Installation:"},{"location":"install/#deploy","text":"After pulling the latest image, it is recommended to use the CLI for launching of each image as checks for necessary environment variables are built in to the interface. A full description of each image configuration is described here . Modify cli/nalms for execution: $ chmod +x cli/nalms","title":"Deploy"},{"location":"install/#configuration","text":"This sections includes an attempt to address configuration items, giving some insight to the service configuration within the Dockerized components and their Docker arguments. An attempt has been made to sufficiently abstract scripts for deployments based on environment variables and a full descriptions of a complete environment for deployment is giving in the CLI documentation. Below, each component is outline with respect to Docker configuration variables and configuration file structure. For a full resource of available configurations, the source documentation will be linked.","title":"Configuration"},{"location":"install/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"install/#configuration_1","text":"The Elasticsearch configuration consists of three main files: elasticsearch.yml for configuring Elasticsearch jvm.options for configuring Elasticsearch JVM settings log4j2.properties for configuring Elasticsearch logging A reference for elasticsearch configuration files can be found here . In order for the Elasticsearch fields to be properly formatted, a template matching the topic scheme must be posted to the server. These may be versioned and are automatically applied to newly created indices. The initial script for templating NALMS topics is hosted in elasticsearch/scripts/create_alarm_template.sh . This template has been taken from the Phoebus source examples .","title":"Configuration"},{"location":"install/#docker","text":"The elasticsearch node may be configured using an exposed port, node specific variables, and Kafka networking variables. Because this is a single node deployment, single-node deployment is used. Java options may be specifified using the ES_JAVA_OPTS variable. The Elasticsearch docker image also creates the appropriate elasticsearch template for the configuration messages. The configuration files must be mounted to /usr/share/elasticsearch/config . The following Docker run command will lauch an Elasticsearch node reachable on host machine port 9200. $ docker run \\ -e node.name = node01 \\ -e cluster.name = es-cluster-7 \\ -e discovery.type = single-node \\ -e ES_JAVA_OPTS = \"-Xms128m -Xmx128m\" \\ -e ES_HOST = localhost \\ -e ES_PORT = 9200 \\ -v \" ${ NALMS_ES_CONFIG } :/usr/share/elasticsearch/config\" \\ -p \" $NALMS_ES_PORT :9200\" \\ --name nalms_elasticsearch \\ -d tidacs/nalms-elasticsearch:latest","title":"Docker"},{"location":"install/#zookeeper","text":"","title":"Zookeeper"},{"location":"install/#configuration_2","text":"At present, Zookeeper is launched using the default settings. For more sophisticated deployments, a configuration with mounted configuration files would be preferable. The configuration file is mounted to the Zookeeper container at runtime. A description of the zookeeper configuration may be found here .","title":"Configuration"},{"location":"install/#docker_1","text":"The following command will run Zookeeper accessible on the host machine at port 2181: $ docker run -p \" ${ NALMS_ZOOKEEPER_PORT } :2181\" -e ZOOKEEPER_CONFIG = /tmp/zoo.cfg \\ -v \" ${ NALMS_ZOOKEEPER_CONFIG } :/tmp/zoo.cfg\" --name nalms_zookeeper \\ -d tidacs/nalms-zookeeper:latest","title":"Docker"},{"location":"install/#kafka","text":"","title":"Kafka"},{"location":"install/#configuration_3","text":"This file is used to configure general properties of a Kafka broker including replication settings and communications protocols. Listeners are defined with respect to configured protocols and binding ports. Advertised listeners are configured with respect to configured protocol and exposed ports. The replication.factor must be appropriately modified based off of the number of nodes in the deployment. A single broker deployment would require replication.factor set to 1. A cluster deployment can accomodate a larger replication factor across the cluster and this file must be modified for the purpose. Networking configurations for SSL/TLS configuration settings are described here . Also defined in this file is the reference to the zookeeper docker image resource: zookeeper . connect = zookeeper : 2181 Certain configurations options may be defined on the topic level. In phoebus-alarm-server/cli/commands/create-kafka-indices , state topics are created with partitions and replications dependent on the cluster settings. After initial creation, the Talk and Command topics are modified to use the deletion cleanup policy with set retention time. At present, the Talk command unused. The create-kafka-indices command is automatically executed during alarm server docker image startup. There are many other settings pertaining to the optimization of the cluster and must be determined by traffic demands. A full catalog of available configurations may be found in the documentation, here .","title":"Configuration"},{"location":"install/#docker_2","text":"The Kafka broker images require the definition of Kafka networking variables, KAFKA_ADVERTISED_LISTENERS , KAFKA_LISTENER_SECURITY_PROTOCOL_MAP , KAFKA_LISTENERS , ZOOKEEPER_CONNECT and must be provided a numeric broker ID. The image must be provisioned with an 8g memory allocation. Additional optimizations may be performed using the Docker image configurations . A configuration file must be mounted to /opt/kafka/server.properties for the image, with properly formatted networking and replication numbers. An example server configuration is given in examples/demo/config/server.properties . An example run command for the Kafka docker image is given below: $ docker run -m 8g \\ -e KAFKA_ADVERTISED_LISTENERS = PLAINTEXT:// ${ HOST_IP } :9092,CONNECTIONS_FROM_HOST:// ${ HOST_IP } :19092 \\ -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP = PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT \\ -e KAFKA_LISTENERS = PLAINTEXT:// ${ HOST_IP } :9092,CONNECTIONS_FROM_HOST://0.0.0.0:19092 \\ -e ZOOKEEPER_CONNECT = ${ HOST_IP } :2182 \\ -e BROKER_ID = 0 \\ -v \"/full/path/to/examples/demo/config/server.properties:/opt/kafka/server.properties\" \\ -p \"19092:19092\" \\ --name nalms_kafka_0 \\ -d tidacs/nalms-kafka:latest Instructions on configuring the Docker image with SSL are given in networking .","title":"Docker"},{"location":"install/#phoebus-alarm-server","text":"","title":"Phoebus Alarm Server"},{"location":"install/#configuration_4","text":"The Phoebus alarm server configuration properties file defines the EPICS configuration and Elasticsearch host configuration. A full preference list can be found in the CS-Studio documentation . Of particular importance are the EPICS, Kakfa, and Elasticsearch properties. The Alarm Server requires: # Channel access settings org.phoebus.pv.ca/addr_list = localhost:5064 org.phoebus.pv.ca/server_port = 5064 org.phoebus.pv.ca/repeater_port = 5065 org.phoebus.pv.ca/auto_addr_list = no # pvAccess settings # Kafka org.phoebus.applications.alarm/server = kafka:19092","title":"Configuration"},{"location":"install/#docker_3","text":"The Phoebus alarm server requires mounting of the configuration file with the Docker volume option and the definition of environment variables indicating Kafka networking address, whether the alarm IOC is to be used, and the EPICS configuration settings to access the alarm and variable iocs. The image supports the substitution of networking variables ($KAFKA_BOOTSTRAP and EPICS variables). Alternatively, these can be defined directly in the configuration file. The Docker run command for the packaged example is given below: $ docker run -v $CONFIG_FILE :/tmp/nalms/ $CONFIG_NAME .xml \\ --name nalms_server_ $CONFIG_NAME \\ -v \" ${ NALMS_ALARM_SERVER_PROPERTIES } :/opt/nalms/config/alarm_server.properties\" \\ -e ALARM_IOC = false \\ -e KAFKA_BOOTSTRAP = \" ${ NALMS_KAFKA_BOOTSTRAP } \" \\ -e EPICS_CA_ADDR_LIST = \" ${ EPICS_CA_ADDR_LIST } \" \\ -e EPICS_CA_SERVER_PORT = \" ${ EPICS_CA_SERVER_PORT } \" \\ -e EPICS_CA_REPEATER_PORT = \" ${ EPICS_CA_REPEATER_PORT } \" \\ -e EPICS_PVA_ADDR_LIST = \" ${ EPICS_PVA_ADDR_LIST } \" \\ -e EPICS_PVA_SERVER_PORT = \" ${ EPICS_PVA_SERVER_PORT } \" \\ -e EPICS_PVA_REPEATER_PORT = \" ${ EPICS_PVA_REPEATER_PORT } \" \\ -e ALARM_SERVER_PROPERTIES = \"/opt/nalms/config/alarm_server.properties\" \\ -d -t tidacs/nalms-phoebus-alarm-server:latest start-server $CONFIG_NAME /tmp/nalms/ $CONFIG_NAME .xml The configuration file must be mounted to `/tmp/nalms/${CONFIG_NAME}, for internal identification.","title":"Docker"},{"location":"install/#phoebus-alarm-logger","text":"","title":"Phoebus Alarm Logger"},{"location":"install/#configuration_5","text":"The alarm logger properties file requires the definition of Elasticsearch and Kafka networking environment variables. The templated file used by the image is hosted at phoebus-alarm-logger/logger.properties . # location of elastic node/s es_host = localhost es_port = 9200 # Kafka server location bootstrap.servers = localhost:9092 Additionally, logging for the logger is configurable and defined in phoebus-alarm-server/logger.properties .","title":"Configuration"},{"location":"install/#docker_4","text":"The Phoebus alarm logger requires the mounting of the configuration file with the Docker volume option. The image supports the interpolation of networking variables $NALMS_ES_HOST, $NALMS_ES_PORT, and $NALMS_KAFKA_BOOTSTRAP in this file. The Docker run command for the packaged example is given below: $ docker run -v $CONFIG_FILE :/tmp/nalms/ $CONFIG_NAME .xml \\ -e ES_HOST = \" ${ NALMS_ES_HOST } \" \\ -e ES_PORT = \" ${ NALMS_ES_PORT } \" \\ -e BOOTSTRAP_SERVERS = \" ${ NALMS_KAFKA_BOOTSTRAP } \" \\ -e ALARM_LOGGER_PROPERTIES = \"/opt/nalms/config/alarm_logger.properties\" \\ -v \" ${ ALARM_LOGGER_PROPERTIES } :/opt/nalms/config/alarm_logger.properties\" \\ --name nalms_logger_ $CONFIG_NAME \\ -d tidacs/nalms-phoebus-alarm-logger:latest start-logger $CONFIG_NAME /tmp/nalms/ $CONFIG_NAME .xml The configuration file must be mounted to `/tmp/nalms/${CONFIG_NAME}, for internal identification.","title":"Docker"},{"location":"install/#phoebus-client","text":"","title":"Phoebus Client"},{"location":"install/#configuration_6","text":"Like the alarm server and logger, the client also accepts a properties file that defines networking: here .","title":"Configuration"},{"location":"install/#grafana","text":"","title":"Grafana"},{"location":"install/#configuration_7","text":"Grafana datasources and dashboards may be programatically provisioned as outlined here . Elasticsearch datasources define an index and networking variables. General Grafana configuration is described here . The dashboard template is hosted at grafana/dashboards/alarm_logs_dashboard.json and a configuration dashboard can be created using the cli/nalms build-grafana-dashboard config-name command. The datasource may be added to an existing datasource file using the cli/nalms add-grafana-datasource config-name command or manually created.","title":"Configuration"},{"location":"install/#docker_5","text":"The Grafana image requires mounting of the dashboards, datasource file, and configuration file. The Docker run command for the packaged example is given below: $ docker run \\ -p \" ${ NALMS_GRAFANA_PORT } :3000\" \\ -v \" ${ NALMS_GRAFANA_DASHBOARD_DIR } :/var/lib/grafana/dashboards\" \\ -v \" ${ NALMS_GRAFANA_DATASOURCE_FILE } :/etc/grafana/provisioning/datasources/all.yml\" \\ -v \" ${ NALMS_GRAFANA_CONFIG } :/etc/grafana/config.ini\" \\ -e ES_HOST = $NALMS_ES_HOST \\ -e ES_PORT = $NALMS_ES_PORT \\ --name nalms_grafana \\ -d tidacs/nalms-grafana:latest The datasource file must be mounted to /etc/grafana/provisioning/datasources/all.yml , the dashboard directory must be mounted to /var/lib/grafana/dashboards , and the configuration must be mounted to /etc/grafana/provisioning/datasources/all.yml . The Grafana dashboards are then reachable at localhost:${NALMS_GRAFANA_PORT} in browser.","title":"Docker"},{"location":"install/#cruise-control","text":"","title":"Cruise Control"},{"location":"install/#configuration_8","text":"The cruise-control/cruisecontrol.properties file dictates the behavior of the cruise control server, allowing definition of relevant thresholds and networking nodes. The tidacs/nalms-cruise-control image performs interpolation on this file in order to pass the relevant environment variables. See wiki: https://github.com/linkedin/cruise-control/wiki https://github.com/linkedin/cruise-control-ui/wiki/Single-Kafka-Cluster","title":"Configuration"},{"location":"install/#docker_6","text":"In order to run this image, you must mount a cruisecontrol.properties to a path specified with the $CRUISE_CONTROL_PROPERTIES env variable. The image will perform interpolation on properties files with $BOOTSTRAP_SERVERS or $ZOOKEEPER_CONNECT as placeholders and defined $BOOTSTRAP_SERVERS or $ZOOKEEPER_CONNECT environment variables. The Docker run command for the packaged example is given below: $ docker run \\ -e BOOTSTRAP_SERVERS = \" ${ NALMS_KAFKA_BOOTSTRAP } \" \\ -e ZOOKEEPER_CONNECT = \" ${ NALMS_ZOOKEEPER_HOST } : ${ NALMS_ZOOKEEPER_PORT } \" \\ -e CRUISE_CONTROL_PROPERTIES = \"/opt/cruise-control/config/cruisecontrol.properties\" \\ -v \" ${ NALMS_CRUISE_CONTROL_PROPERTIES } :/opt/cruise-control/config/cruisecontrol.properties\" \\ --name nalms_cruise_control \\ -p \" $NALMS_CRUISE_CONTROL_PORT :9090\" -d tidacs/nalms-cruise-control:latest The Cruise Control UI is then available in browser at localhost:9090.","title":"Docker"},{"location":"kafka/","text":"Kafka Apache Kafka will be used to maintain alarm state messages, alarm hierarchy configuration, and for the communication of commands between the client and alarm server. Kafka uses a publish/subscribe model to synchronize state between data producers and data ingestors. Messages are constructed using a key-value structure. Producers write events to Kafka servers (brokers) using categorized messages termed topics. Topic messages are divided into a designated number of bucketed partitions, defined using the message keys. These partitions are replicated across brokers, with one replica elected as the leader responsible for the reading/writing of a topic. In the case of leader broker failure, a replica becomes leader. Leaders write new messages to other replicas and reads/writes are consequently parallelized across the cluster. Kafka brokers are synchronized by a Zookeeper node. This Zookeeper node is responsible for leader election, maintaining a registry of cluster members, configuring topics (number of partitions, leader location, etc.), and access control. We will use the Zookeeper packaged with the Kafka distribution until its deprecation. The Kafka broker configuration file assigns an id for the broker, communication protocols, partition count, compaction settings and others. The latest Kafka version as of writing (2.8.1) will be used out of the box, with configuration options tailored to frequent log compaction for state maintenance. The release of Kafka Improvement Proposal 500 at some point in 2021 warrants version reconsideration, as the new deprecation of zookeeper will allow the removal of the dedicated server and move to a self managed quorum model. The NALMS production Kafka cluster consists of three nodes, topic deletion enabled, with compaction cleanup policy for state messages and deletion for configuration and commands and frequent cleanup operations (max lag 1s). A development cluster consists of only a single node, hosted locally. A full description of configuration options is provided in the Apache Kafka documentation . Each broker may be configured with a keystore and truststore for SSL authentication and encryption. Kafka messages Categorized Kafka messages facilitate all interactions with the alarm server. Alarm events are translated into Kafka messages by the alarm server (state topic), commands are communicated to the alarm server (command topic), and configurations are defined and manipulated (config topic). The key-value structure of the Kafka messages maintains the alarm hierarchy. Keys are prefixed with \u201ccommand\u201d, \u201cstate\u201d, or \u201cconfig\u201d, and represent the full alarm item path as forward slash delineated locations in the hierarchy. For example, KLYS:LI23:21:DL_WG_TEMP in the following tree would be indicated by the path: /Temp/KLYS/KLYS:LI23:21/KLYS:LI23:21:DL_WG_TEMP. Temp \u2514\u2500\u2500 KLYS \u251c\u2500\u2500 KLYS:LI23:11 \u2502 \u2514\u2500\u2500 KLYS:LI23:11:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:21 \u2502 \u2514\u2500\u2500 KLYS:LI23:21:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:31 \u2502 \u2514\u2500\u2500 KLYS:LI23:31:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:41 \u2502 \u2514\u2500\u2500 KLYS:LI23:41:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:51 \u2502 \u2514\u2500\u2500 KLYS:LI23:51:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:61 \u2502 \u2514\u2500\u2500 KLYS:LI23:61:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:71 \u2502 \u2514\u2500\u2500 KLYS:LI23:71:DL_WG_TEMP \u2514\u2500\u2500 KLYS:LI23:81 \u2514\u2500\u2500 KLYS:LI23:81:DL_WG_TEMP The Kafka configuration message for the PV would be keyed by the string config:/Temp/KLYS/KLYS:LI23:11/KLYS:LI23:11:DL_WG . Associated values are JSON representations of the associated values. Representations for alarm tree leaves and nodes are outlined below. Undefined elements are omitted in practice. Alarm leaf configuration { \"user\" : S tr i n g , \"host\" : S tr i n g , \"description\" : S tr i n g , \"delay\" : I nte ger , \"count\" : I nte ger , \"filter\" : S tr i n g , \"guidance\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"displays\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"commands\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"actions\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }] } Alarm node configuration { \"user\" : S tr i n g , \"host\" : S tr i n g , \"guidance\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"displays\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"commands\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"actions\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }] } Alarm leaf state { \"severity\" : S tr i n g , \"latch\" : Boolea n , \"message\" : S tr i n g , \"value\" : S tr i n g , \"time\" : { \"seconds\" : Lo n g , \"nano\" : Lo n g }, \"current_severity\" : S tr i n g , \"current_message\" : S tr i n g , \"mode\" : S tr i n g , } Alarm node state { \"severity\" : S tr i n g , \"mode\" : S tr i n g , } Command { \"user\" : S tr i n g , \"host\" : S tr i n g , \"command\" : S tr i n g }","title":"Kafka"},{"location":"kafka/#kafka","text":"Apache Kafka will be used to maintain alarm state messages, alarm hierarchy configuration, and for the communication of commands between the client and alarm server. Kafka uses a publish/subscribe model to synchronize state between data producers and data ingestors. Messages are constructed using a key-value structure. Producers write events to Kafka servers (brokers) using categorized messages termed topics. Topic messages are divided into a designated number of bucketed partitions, defined using the message keys. These partitions are replicated across brokers, with one replica elected as the leader responsible for the reading/writing of a topic. In the case of leader broker failure, a replica becomes leader. Leaders write new messages to other replicas and reads/writes are consequently parallelized across the cluster. Kafka brokers are synchronized by a Zookeeper node. This Zookeeper node is responsible for leader election, maintaining a registry of cluster members, configuring topics (number of partitions, leader location, etc.), and access control. We will use the Zookeeper packaged with the Kafka distribution until its deprecation. The Kafka broker configuration file assigns an id for the broker, communication protocols, partition count, compaction settings and others. The latest Kafka version as of writing (2.8.1) will be used out of the box, with configuration options tailored to frequent log compaction for state maintenance. The release of Kafka Improvement Proposal 500 at some point in 2021 warrants version reconsideration, as the new deprecation of zookeeper will allow the removal of the dedicated server and move to a self managed quorum model. The NALMS production Kafka cluster consists of three nodes, topic deletion enabled, with compaction cleanup policy for state messages and deletion for configuration and commands and frequent cleanup operations (max lag 1s). A development cluster consists of only a single node, hosted locally. A full description of configuration options is provided in the Apache Kafka documentation . Each broker may be configured with a keystore and truststore for SSL authentication and encryption.","title":"Kafka"},{"location":"kafka/#kafka-messages","text":"Categorized Kafka messages facilitate all interactions with the alarm server. Alarm events are translated into Kafka messages by the alarm server (state topic), commands are communicated to the alarm server (command topic), and configurations are defined and manipulated (config topic). The key-value structure of the Kafka messages maintains the alarm hierarchy. Keys are prefixed with \u201ccommand\u201d, \u201cstate\u201d, or \u201cconfig\u201d, and represent the full alarm item path as forward slash delineated locations in the hierarchy. For example, KLYS:LI23:21:DL_WG_TEMP in the following tree would be indicated by the path: /Temp/KLYS/KLYS:LI23:21/KLYS:LI23:21:DL_WG_TEMP. Temp \u2514\u2500\u2500 KLYS \u251c\u2500\u2500 KLYS:LI23:11 \u2502 \u2514\u2500\u2500 KLYS:LI23:11:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:21 \u2502 \u2514\u2500\u2500 KLYS:LI23:21:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:31 \u2502 \u2514\u2500\u2500 KLYS:LI23:31:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:41 \u2502 \u2514\u2500\u2500 KLYS:LI23:41:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:51 \u2502 \u2514\u2500\u2500 KLYS:LI23:51:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:61 \u2502 \u2514\u2500\u2500 KLYS:LI23:61:DL_WG_TEMP \u251c\u2500\u2500 KLYS:LI23:71 \u2502 \u2514\u2500\u2500 KLYS:LI23:71:DL_WG_TEMP \u2514\u2500\u2500 KLYS:LI23:81 \u2514\u2500\u2500 KLYS:LI23:81:DL_WG_TEMP The Kafka configuration message for the PV would be keyed by the string config:/Temp/KLYS/KLYS:LI23:11/KLYS:LI23:11:DL_WG . Associated values are JSON representations of the associated values. Representations for alarm tree leaves and nodes are outlined below. Undefined elements are omitted in practice.","title":"Kafka messages"},{"location":"kafka/#alarm-leaf-configuration","text":"{ \"user\" : S tr i n g , \"host\" : S tr i n g , \"description\" : S tr i n g , \"delay\" : I nte ger , \"count\" : I nte ger , \"filter\" : S tr i n g , \"guidance\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"displays\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"commands\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"actions\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }] }","title":"Alarm leaf configuration"},{"location":"kafka/#alarm-node-configuration","text":"{ \"user\" : S tr i n g , \"host\" : S tr i n g , \"guidance\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"displays\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"commands\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }], \"actions\" : [{ \"title\" : S tr i n g , \"details\" : S tr i n g }] }","title":"Alarm node configuration"},{"location":"kafka/#alarm-leaf-state","text":"{ \"severity\" : S tr i n g , \"latch\" : Boolea n , \"message\" : S tr i n g , \"value\" : S tr i n g , \"time\" : { \"seconds\" : Lo n g , \"nano\" : Lo n g }, \"current_severity\" : S tr i n g , \"current_message\" : S tr i n g , \"mode\" : S tr i n g , }","title":"Alarm leaf state"},{"location":"kafka/#alarm-node-state","text":"{ \"severity\" : S tr i n g , \"mode\" : S tr i n g , }","title":"Alarm node state"},{"location":"kafka/#command","text":"{ \"user\" : S tr i n g , \"host\" : S tr i n g , \"command\" : S tr i n g }","title":"Command"},{"location":"legacy/","text":"Converting from ALH The ALH -> Phoebus Python (>=3.7) conversion tool is handled by a package currently defined in the nalms-tools directory. $ cd nalms-tools $ pip install -e . The entry point console script is then available with: $ convert-alh config_name input_filename output_filename Several features of the ALH cannot be translated to Phoebus configurations and are deprecated in NALMS. These are the ALIAS, ACKPV, SEVRCOMMAND, STATCOMMAND, and BEEPSEVERITY ALH configuration entries. The conversion script will print any failures to STDOUT. At present, ALH inclusions will parsed and reserialized into a single Phoebus XML configuration file. Future CS-Studio development with include the ability to accomodate file inclusions within the tree structure such that nested files may be similarly structured. Subsystem demo In this demo we will convert an existing ALH configuration file into a suitable Phoebus XML configuration file using the packaged nalms CLI. This demo is written to run using the development environment on aird-b50-srv01 and assumes already running kafka cluster, elasticsearch, and Grafana. Source the appropriate environment: $ source ${ PACKAGE_TOP } /nalms/setup/aird-b50-srv01/demo.env Use the top level subsystem ALH config file to create the XML file: $ nalms convert-alh ${ TOOLS } /AlarmConfigsTop/mgnt/prod/lcls/alh/mgnt.alhConfig mgnt.xml Mgnt Launch the Phoebus alarm server for the configuration: # must use full path to the configuration file $ nalms start-alarm-server Mgnt $(pwd)/mgnt.xml Launch the Phoebus alarm logger for the configuration: # must use full path to the configuration file $ nalms start-alarm-logger Mgnt $(pwd)/mgnt.xml Launch the client to view the alarm tree: $ nalms start-phoebus-client Mgnt Now, set up for use with Grafana. Add to the Grafana datasource: $ nalms add-grafana-datasource Mgnt This appended the datasource to the file $NALMS_GRAFANA_DATASOURCE_FILE. Now, create the Grafana dashboard: $ nalms build-grafana-dashboard Mgnt This created a dashboard for the Demo configuration in $NALMS_GRAFANA_DASHBOARD_DIR. Relaunch grafana: $ docker container stop nalms_grafana $ docker container rm nalms_grafana $ nalms start-grafana You will now see the dashboard available at the Grafana instance at http://localhost:3000.","title":"Legacy"},{"location":"legacy/#converting-from-alh","text":"The ALH -> Phoebus Python (>=3.7) conversion tool is handled by a package currently defined in the nalms-tools directory. $ cd nalms-tools $ pip install -e . The entry point console script is then available with: $ convert-alh config_name input_filename output_filename Several features of the ALH cannot be translated to Phoebus configurations and are deprecated in NALMS. These are the ALIAS, ACKPV, SEVRCOMMAND, STATCOMMAND, and BEEPSEVERITY ALH configuration entries. The conversion script will print any failures to STDOUT. At present, ALH inclusions will parsed and reserialized into a single Phoebus XML configuration file. Future CS-Studio development with include the ability to accomodate file inclusions within the tree structure such that nested files may be similarly structured.","title":"Converting from ALH"},{"location":"legacy/#subsystem-demo","text":"In this demo we will convert an existing ALH configuration file into a suitable Phoebus XML configuration file using the packaged nalms CLI. This demo is written to run using the development environment on aird-b50-srv01 and assumes already running kafka cluster, elasticsearch, and Grafana. Source the appropriate environment: $ source ${ PACKAGE_TOP } /nalms/setup/aird-b50-srv01/demo.env Use the top level subsystem ALH config file to create the XML file: $ nalms convert-alh ${ TOOLS } /AlarmConfigsTop/mgnt/prod/lcls/alh/mgnt.alhConfig mgnt.xml Mgnt Launch the Phoebus alarm server for the configuration: # must use full path to the configuration file $ nalms start-alarm-server Mgnt $(pwd)/mgnt.xml Launch the Phoebus alarm logger for the configuration: # must use full path to the configuration file $ nalms start-alarm-logger Mgnt $(pwd)/mgnt.xml Launch the client to view the alarm tree: $ nalms start-phoebus-client Mgnt Now, set up for use with Grafana. Add to the Grafana datasource: $ nalms add-grafana-datasource Mgnt This appended the datasource to the file $NALMS_GRAFANA_DATASOURCE_FILE. Now, create the Grafana dashboard: $ nalms build-grafana-dashboard Mgnt This created a dashboard for the Demo configuration in $NALMS_GRAFANA_DASHBOARD_DIR. Relaunch grafana: $ docker container stop nalms_grafana $ docker container rm nalms_grafana $ nalms start-grafana You will now see the dashboard available at the Grafana instance at http://localhost:3000.","title":"Subsystem demo"},{"location":"networking/","text":"Kafka SSL The Kafka docker image defined in this repository has been configured to run with SSL enabled or not, indicated by the USE_SSL=true environment variable. For SSL use, it is necessary to mount a configuration file with the following relevant items defined in server.properties to the /opt/kafka/server.properties : ssl.truststore.location=/opt/kafka/ssl/server.truststore.jks ssl.keystore.location=/opt/kafka/ssl/server.keystore.jks security.inter.broker.protocol=SSL ssl.client.auth=requested ssl.keystore.type=JKS ssl.endpoint.identification.algorithm= Additionally, the listener security protocol map defined in the environment variables must be reflect outgoing SSL messages. For example: KAFKA_ADVERTISED_LISTENERS : SSL :// kafka . broker1 : 9092 , CONNECTIONS_FROM_HOST :// localhost : 19093 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP : SSL : SSL , CONNECTIONS_FROM_HOST : PLAINTEXT KAFKA_LISTENERS : SSL :// kafka . broker1 : 9092 , CONNECTIONS_FROM_HOST :// 0.0 . 0.0 : 19093 Relevant passwords must also be passed: TRUSTSTORE_PASSWORD : kafkabroker KEYSTORE_PASSWORD : kafkabroker KEY_PASSWORD : kafkabroker A utility script for generating the truststore/keystore can be run: $ bash cli/nalms generate-kafka-certs domain password This utility might be decomposed further into truststore/keystore/key passwords. The appropriate keystore will then be mounted to the docker volume at /opt/kafka/ssl . Keys for each broker will need to be added to the respective trust stores of each broker node.Documentation on SSL for Kafka may be found here . Instructions for configuring the Kafka truststore may be found here Phoebus The Phoebus alarm server and logger to not accomodate SSL/TLS out of the box and will require development. The workflow that must be changed to accomodate SSL on the Phoebus side can be found in the following file: phoebus/app/alarm/model/src/main/java/org/phoebus/applications/alarm/client/KafkaHelper.java . Logically, this will mean exposing the following additional streams settings to the application: security.protocol=SSL ssl.truststore.location=/path/to/kafka.client.truststore.jks ssl.truststore.password=truststore_password ssl.keystore.location=/path/to/kafka.client.keystore.jks ssl.keystore.password=keystore_password ssl.key.password=key_password More information for setting up these settings may be found here . Elasticsearch Instructions for configuring elasticsearch security may be found here: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/ssl-tls.html The Docker image provided with this repository is based off of the official Elasticsearch 6.8 image and the following guide can be used to configure SSL/TLS with this image: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/docker.html Grafana Grafana Elasticsearch datasources may be configured to use certificates during setup. Options for provisioning datasources may be found here: https://grafana.com/docs/grafana/latest/administration/provisioning/ PyDM The PyDM datasource and client widgets will need to be built to accomodate authentication. See project board here: https://github.com/jacquelinegarrahan/pydm/projects/1?add_cards_query=is%3Aopen","title":"Networking"},{"location":"networking/#kafka-ssl","text":"The Kafka docker image defined in this repository has been configured to run with SSL enabled or not, indicated by the USE_SSL=true environment variable. For SSL use, it is necessary to mount a configuration file with the following relevant items defined in server.properties to the /opt/kafka/server.properties : ssl.truststore.location=/opt/kafka/ssl/server.truststore.jks ssl.keystore.location=/opt/kafka/ssl/server.keystore.jks security.inter.broker.protocol=SSL ssl.client.auth=requested ssl.keystore.type=JKS ssl.endpoint.identification.algorithm= Additionally, the listener security protocol map defined in the environment variables must be reflect outgoing SSL messages. For example: KAFKA_ADVERTISED_LISTENERS : SSL :// kafka . broker1 : 9092 , CONNECTIONS_FROM_HOST :// localhost : 19093 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP : SSL : SSL , CONNECTIONS_FROM_HOST : PLAINTEXT KAFKA_LISTENERS : SSL :// kafka . broker1 : 9092 , CONNECTIONS_FROM_HOST :// 0.0 . 0.0 : 19093 Relevant passwords must also be passed: TRUSTSTORE_PASSWORD : kafkabroker KEYSTORE_PASSWORD : kafkabroker KEY_PASSWORD : kafkabroker A utility script for generating the truststore/keystore can be run: $ bash cli/nalms generate-kafka-certs domain password This utility might be decomposed further into truststore/keystore/key passwords. The appropriate keystore will then be mounted to the docker volume at /opt/kafka/ssl . Keys for each broker will need to be added to the respective trust stores of each broker node.Documentation on SSL for Kafka may be found here . Instructions for configuring the Kafka truststore may be found here","title":"Kafka SSL"},{"location":"networking/#phoebus","text":"The Phoebus alarm server and logger to not accomodate SSL/TLS out of the box and will require development. The workflow that must be changed to accomodate SSL on the Phoebus side can be found in the following file: phoebus/app/alarm/model/src/main/java/org/phoebus/applications/alarm/client/KafkaHelper.java . Logically, this will mean exposing the following additional streams settings to the application: security.protocol=SSL ssl.truststore.location=/path/to/kafka.client.truststore.jks ssl.truststore.password=truststore_password ssl.keystore.location=/path/to/kafka.client.keystore.jks ssl.keystore.password=keystore_password ssl.key.password=key_password More information for setting up these settings may be found here .","title":"Phoebus"},{"location":"networking/#elasticsearch","text":"Instructions for configuring elasticsearch security may be found here: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/ssl-tls.html The Docker image provided with this repository is based off of the official Elasticsearch 6.8 image and the following guide can be used to configure SSL/TLS with this image: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/docker.html","title":"Elasticsearch"},{"location":"networking/#grafana","text":"Grafana Elasticsearch datasources may be configured to use certificates during setup. Options for provisioning datasources may be found here: https://grafana.com/docs/grafana/latest/administration/provisioning/","title":"Grafana"},{"location":"networking/#pydm","text":"The PyDM datasource and client widgets will need to be built to accomodate authentication. See project board here: https://github.com/jacquelinegarrahan/pydm/projects/1?add_cards_query=is%3Aopen","title":"PyDM"},{"location":"phoebus/","text":"Phoebus Alarm Server For the translation of IOC alarm state to Kafka message and the handling of configuration and command representations, the new alarm system will use the CS-Studio Collaboration Phoebus alarm server. Alarm updates are configurable for receipt over Channel Access and pvAccess using either the \u201cca://\u201d or \u201cpva://\u201d prefix in the PV name configuration, respectively. Both may be used in a single configuration. The CS-Studio community has approved the following features for development: The configuration topic schema will be extended to include a variable number of miscellaneous tags The XML parsing model must be modified to accommodate nested inclusions The Alarm erver functionality must be extended to allow for the assignment of an expiration date for alarm bypasses. Documentation for the Phoebus Alarm Server may be found here . Configuration files Alarm configurations are XML files organized with component (group), and PV tags. Component tags accept specifications for guidance, display, commands, and automated actions. Configuration options for groups are defined below: Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. PV tags accept specifications for enabling, latching, annunciating, description, delay, commands, associated displays, guidance, alarm count, filter, and automated actions. A configuration schema is provided here . Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. description Text displayed in the alarm table when the alarm is triggered delay Alarm will be triggered if the PV remains in alarm for at least this time enabled If false, ignore the value of this PV latching Alarms will latch to the highest severity until the alarm is acknowledged and cleared. If false, alarm may recover without requiring acknowledgment count If the trigger PV exhibits a not-OK alarm severity for more than \u2018count\u2019 times within the alarm delay, recognize the alarm filter An optional expression that can enable the alarm based on other PVs. Phoebus Alarm Logger The Phoebus alarm logger is used for the translation of Kafka messages to Elasticsearch. While multiple configurations may be grouped into a single logger, this consolidation may lead to logging outages in the event of configuration deprecation or configuration deployment. For this reason, it may be suitable to run a designated alarm logger instance per configuration.\u202f Elasticsearch is a search engine build over the widely used Apache Lucene library, a Java-based search and indexing tool. Elasticsearch manages Lucene at scale, managing indices in a distributed fashion and providing additional data management and access features. JSON documents are written to an Elasticsearch server where they are tokenized, analyzed, and stored alongside indexed data representations of field values. Elasticsearch indices are created for the alarm events using following schemes, using the creation date of the index: {config_name}_alarms_state_yyyy_mm_dd, {config_name}_alarms_config_yyyy_mm_dd, and {config_name}_alarms_cmd_yyyy_mm_dd . Indices are duration based using a default of one month. This may be reduced or extended based on volume. Elasticsearch supports aggregated metrics accessible via query including percentiles, summations, and averages. Custom expressions may be evaluated using the packaged scripting API, written in the painless language, or by building a custom Java plugin. Index aliasing may be a more effective way to handle the indices, with rollover of old indices automated, to avoid implicit datecoding of log names. More sophisticated elasticsearch lifecycle management should be explored and the X-pack enterprise option considered. Index Field {config_name}_alarms_state_yyyy_mm_dd config (pv path with topic prefix) current_message current_severity latch message message_time mode notify pv severity time value {config_name}_alarms_config_yyyy_mm_dd config config_msg enabled host latch message user {config_name}_alarms_cmd_yyyy_mm_dd command config host message_time user","title":"Phoebus Alarm Tools"},{"location":"phoebus/#phoebus-alarm-server","text":"For the translation of IOC alarm state to Kafka message and the handling of configuration and command representations, the new alarm system will use the CS-Studio Collaboration Phoebus alarm server. Alarm updates are configurable for receipt over Channel Access and pvAccess using either the \u201cca://\u201d or \u201cpva://\u201d prefix in the PV name configuration, respectively. Both may be used in a single configuration. The CS-Studio community has approved the following features for development: The configuration topic schema will be extended to include a variable number of miscellaneous tags The XML parsing model must be modified to accommodate nested inclusions The Alarm erver functionality must be extended to allow for the assignment of an expiration date for alarm bypasses. Documentation for the Phoebus Alarm Server may be found here .","title":"Phoebus Alarm Server"},{"location":"phoebus/#configuration-files","text":"Alarm configurations are XML files organized with component (group), and PV tags. Component tags accept specifications for guidance, display, commands, and automated actions. Configuration options for groups are defined below: Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. PV tags accept specifications for enabling, latching, annunciating, description, delay, commands, associated displays, guidance, alarm count, filter, and automated actions. A configuration schema is provided here . Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. description Text displayed in the alarm table when the alarm is triggered delay Alarm will be triggered if the PV remains in alarm for at least this time enabled If false, ignore the value of this PV latching Alarms will latch to the highest severity until the alarm is acknowledged and cleared. If false, alarm may recover without requiring acknowledgment count If the trigger PV exhibits a not-OK alarm severity for more than \u2018count\u2019 times within the alarm delay, recognize the alarm filter An optional expression that can enable the alarm based on other PVs.","title":"Configuration files"},{"location":"phoebus/#phoebus-alarm-logger","text":"The Phoebus alarm logger is used for the translation of Kafka messages to Elasticsearch. While multiple configurations may be grouped into a single logger, this consolidation may lead to logging outages in the event of configuration deprecation or configuration deployment. For this reason, it may be suitable to run a designated alarm logger instance per configuration.\u202f Elasticsearch is a search engine build over the widely used Apache Lucene library, a Java-based search and indexing tool. Elasticsearch manages Lucene at scale, managing indices in a distributed fashion and providing additional data management and access features. JSON documents are written to an Elasticsearch server where they are tokenized, analyzed, and stored alongside indexed data representations of field values. Elasticsearch indices are created for the alarm events using following schemes, using the creation date of the index: {config_name}_alarms_state_yyyy_mm_dd, {config_name}_alarms_config_yyyy_mm_dd, and {config_name}_alarms_cmd_yyyy_mm_dd . Indices are duration based using a default of one month. This may be reduced or extended based on volume. Elasticsearch supports aggregated metrics accessible via query including percentiles, summations, and averages. Custom expressions may be evaluated using the packaged scripting API, written in the painless language, or by building a custom Java plugin. Index aliasing may be a more effective way to handle the indices, with rollover of old indices automated, to avoid implicit datecoding of log names. More sophisticated elasticsearch lifecycle management should be explored and the X-pack enterprise option considered. Index Field {config_name}_alarms_state_yyyy_mm_dd config (pv path with topic prefix) current_message current_severity latch message message_time mode notify pv severity time value {config_name}_alarms_config_yyyy_mm_dd config config_msg enabled host latch message user {config_name}_alarms_cmd_yyyy_mm_dd command config host message_time user","title":"Phoebus Alarm Logger"},{"location":"tools/","text":"Tools Alarm Configuration Editor Tool The alarm configuration editor is be a PyQt tool for designing the alarm configuration XML files for use with the Phoebus alarm server as outlined in Section 3.2. Alternatively, any XML editor may be used to build the document directly. The editor has the following features:\u202f * Ability to edit alarm hierarchy, create new groups and new PVs * Ability to define all configuration items * Optional conversion and import of legacy ALH files Requirements for running the editor are given in the environment.yml file bundled with the NALMS package. This environment can be created with conda using: $ conda env create -f environment.yml And subsequently activated: $ conda activate nalms If choosing to build your own environment without conda, the requirements follow: - python =3.8 - treelib - lxml - pyqt5 - kafka-python - pydm PyDM dependence will eventually be dropped. To launch the editor run: $ bash cli/nalms launch-editor PyDM widgets PyDM widgets are in the development stage and relevant code is hosted in pydm-nalms . The integration of the datasource with PyDM is largely dependent upon the development of entrypoints for datasources. This feature request is still open and therefore intermediate use will require modifying PyDM directly or monkeypatching... NALMS-tools ALH Conversion The ALH conversion tool is a python script for the conversion of legacy Alarm Handler Configuration tools. This script provides a command line interface for the purpose of translating an indicated Alarm Handler configuration file into a Phoebus alarm server -compatible XML configuration file, recursively iterating over its inclusions. Further, a report will be generated during execution describing the mapping of original to translated filenames and omissions of incompatible configuration elements. Of the configuration elements defined by the Alarm Handler, ALIAS, ACKPV, SEVRCOMMAND, STATCOMMAND, and BEEPSEVERITY have no analogs in the Phoebus alarm server.\u202f It is assumed that SEVRCOMMAND and STATCOMMAND will be handled inside the alarm system logic as justified in Section 3.2.1. BEEPSEVERITY, ACKPV, and ALIAS functionality will be deprecated. At SLAC, survey of the configuration utilization suggested that the LCLS makes no use of the STATCOMMAND and makes minimal use of the SEVRCOMMAND for sending emails in the case of high severity alarms. For use see legacy .","title":"Tools"},{"location":"tools/#tools","text":"","title":"Tools"},{"location":"tools/#alarm-configuration-editor-tool","text":"The alarm configuration editor is be a PyQt tool for designing the alarm configuration XML files for use with the Phoebus alarm server as outlined in Section 3.2. Alternatively, any XML editor may be used to build the document directly. The editor has the following features:\u202f * Ability to edit alarm hierarchy, create new groups and new PVs * Ability to define all configuration items * Optional conversion and import of legacy ALH files Requirements for running the editor are given in the environment.yml file bundled with the NALMS package. This environment can be created with conda using: $ conda env create -f environment.yml And subsequently activated: $ conda activate nalms If choosing to build your own environment without conda, the requirements follow: - python =3.8 - treelib - lxml - pyqt5 - kafka-python - pydm PyDM dependence will eventually be dropped. To launch the editor run: $ bash cli/nalms launch-editor","title":"Alarm Configuration Editor Tool"},{"location":"tools/#pydm-widgets","text":"PyDM widgets are in the development stage and relevant code is hosted in pydm-nalms . The integration of the datasource with PyDM is largely dependent upon the development of entrypoints for datasources. This feature request is still open and therefore intermediate use will require modifying PyDM directly or monkeypatching...","title":"PyDM widgets"},{"location":"tools/#nalms-tools","text":"","title":"NALMS-tools"},{"location":"tools/#alh-conversion","text":"The ALH conversion tool is a python script for the conversion of legacy Alarm Handler Configuration tools. This script provides a command line interface for the purpose of translating an indicated Alarm Handler configuration file into a Phoebus alarm server -compatible XML configuration file, recursively iterating over its inclusions. Further, a report will be generated during execution describing the mapping of original to translated filenames and omissions of incompatible configuration elements. Of the configuration elements defined by the Alarm Handler, ALIAS, ACKPV, SEVRCOMMAND, STATCOMMAND, and BEEPSEVERITY have no analogs in the Phoebus alarm server.\u202f It is assumed that SEVRCOMMAND and STATCOMMAND will be handled inside the alarm system logic as justified in Section 3.2.1. BEEPSEVERITY, ACKPV, and ALIAS functionality will be deprecated. At SLAC, survey of the configuration utilization suggested that the LCLS makes no use of the STATCOMMAND and makes minimal use of the SEVRCOMMAND for sending emails in the case of high severity alarms. For use see legacy .","title":"ALH Conversion"}]}